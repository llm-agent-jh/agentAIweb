{
  "query_id": "Query1",
  "query_text": "You are an expert AI scientist and architect of a CNAPS‑style multi‑module workflow.  \nHere, CNAPS means a **synapse‑like branching network** of AI models working together—not a simple linear pipeline.\n\nA user asks:\n\"My face is blurry in my travel photos. Please restore it so that my skin tone and contours are naturally clear.\"\n\n**Using ONLY the provided models and papers in the context below, answer the following in full detail.**  \nYou do **not** need to use all listed models—only those that are clearly relevant to the user's goal.\n\n---\n\n## 1. Core Task  \nSummarize what the user wants to achieve in one or two sentences.  \nList any sub-goals involved (e.g., structure preservation, texture realism, style matching).\n\n---\n\n## 2. CNAPS-style Workflow  \nDesign a **high-level synaptic workflow** that clearly shows branching, merging, or conditional paths.  \nYour answer **must include all 3 parts below**:\n\n---\n\n### A. High-Level Overview  \nExplain how the system works in natural language:  \nWhat flows in, how it branches, what gets combined, and what comes out.\n\n---\n\n### B. Visual Flow Diagram (block-style, text-based)  \nUse a simple visual flow chart using boxed steps, arrows, and indentation.  \nUse this style:\n# Here, CNAPS means a **synapse‑like branching network** of AI models working together—not a simple linear pipeline.\n\n# A user asks:\n# \"My face is blurry in my travel photos. Please restore it so that my skin tone and contours are naturally clear.\"\n\n# **Using ONLY the provided models and papers in the context below, do the following:**\n\n# 1. **Identify the core task or goal** implied by the user’s request.  \n# 2. **Design a CNAPS-style synaptic workflow**:\n#    - Describe how input is routed to one or more modules.\n#    - Explain how modules branch, interact, merge, or loop.\n#    - Define each module’s intermediate and final output formats/include examples.\n# 3. **Justify your design** with references to the papers and tools (include GitHub or ArXiv links listed).\n\n# \n### Recommended AI Models:\n- **Deblur-MSSNet-RealBlurR**\n  Paper: https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf\n\n- **Deblur-MSSNet-RealBlurJ**\n  Paper: https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf\n\n- **Deblur-MSSNet-L-GoPro**\n  Paper: https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf\n\n\nAnswer:\n#",
  "responses": {
    "llm_a": "## 1. Core Task\n\nThe user wants to **restore blurry facial features in travel photos to achieve naturally clear skin tones and facial contours**. This involves deblurring the image while preserving realistic facial structure and natural skin appearance.\n\n**Sub-goals:**\n- Motion/blur artifact removal\n- Facial structure preservation \n- Natural skin tone restoration\n- Edge and contour enhancement\n- Texture realism maintenance\n\n---\n\n## 2. CNAPS-style Workflow\n\n### A. High-Level Overview\n\nThe system operates as a **synapse-like branching network** where the input blurry image flows through multiple specialized deblurring pathways simultaneously. Each MSSNet variant processes the image with different blur kernel assumptions and training specializations. The network branches into three parallel processing streams: RealBlurR (for real-world blur patterns), RealBlurJ (for JPEG compression artifacts with blur), and L-GoPro (for motion blur scenarios). Each branch produces its own deblurred interpretation, then these outputs converge through a confidence-weighted fusion mechanism that selectively combines the best aspects of each result based on local image characteristics. The final output emerges from this synaptic integration, producing a naturally restored face with clear skin tones and preserved contours.\n\n### B. Visual Flow Diagram\n\n```\n[Input: Blurry Travel Photo]\n            |\n            v\n    [Image Preprocessing]\n            |\n            |-----> BRANCH 1: [Deblur-MSSNet-RealBlurR]\n            |                      |\n            |                      v\n            |              [Real-world blur removal]\n            |                      |\n            |-----> BRANCH 2: [Deblur-MSSNet-RealBlurJ] \n            |                      |\n            |                      v\n            |              [JPEG artifact + blur removal]\n            |                      |\n            |-----> BRANCH 3: [Deblur-MSSNet-L-GoPro]\n                                   |\n                                   v\n                           [Motion blur removal]\n                                   |\n                                   v\n            [Confidence Map Generation] <---- All branches feed here\n                                   |\n                                   v\n                    [Adaptive Fusion Module]\n                                   |\n                                   v\n                    [Post-processing & Enhancement]\n                                   |\n                                   v\n                [Output: Restored Clear Face Photo]\n```\n\n### C. Detailed Module Specifications\n\n**Branch 1 - RealBlurR Module:**\n- **Input:** RGB image tensor (H×W×3)\n- **Processing:** Multi-scale blur kernel estimation for real-world blur patterns\n- **Output:** Deblurred image + confidence map\n- **Specialization:** Handles natural camera shake and focus blur\n\n**Branch 2 - RealBlurJ Module:**\n- **Input:** Same RGB tensor\n- **Processing:** Joint JPEG compression artifact and blur removal\n- **Output:** Deblurred image + artifact-free regions map\n- **Specialization:** Optimized for compressed travel photos with blur\n\n**Branch 3 - L-GoPro Module:**\n- **Input:** Same RGB tensor  \n- **Processing:** Large kernel motion blur removal\n- **Output:** Sharp image + motion vector field\n- **Specialization:** Handles significant motion blur from handheld photography\n\n**Fusion Module:**\n- **Input:** 3 deblurred images + confidence/quality maps\n- **Processing:** Pixel-wise weighted combination based on local blur characteristics\n- **Output:** Final restored image with natural skin tones and clear contours\n\n---\n\n## 3. Design Justification\n\nThis CNAPS design leverages the **Multi-Scale Stacked Network (MSSNet)** architecture from the referenced paper (https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf), which demonstrates superior performance in handling diverse blur types through its multi-scale approach.\n\n**Key advantages:**\n- **Parallel processing** allows each specialized model to focus on different blur characteristics common in travel photography\n- **RealBlurR and RealBlurJ** variants are specifically trained on realistic blur patterns, making them ideal for travel photos\n- **L-GoPro variant** handles severe motion blur from handheld shots\n- **Synaptic fusion** combines strengths while minimizing individual model weaknesses\n- **Confidence-based merging** ensures natural skin tone preservation by selecting the most reliable restoration for each facial region\n\nThe branching architecture mirrors biological neural networks where multiple pathways process the same input with different specializations, then integrate results for optimal output—perfectly suited for the complex task of facial restoration in diverse blur conditions.",
    "llm_b": "You are an expert AI scientist and architect of a CNAPS‑style multi‑module workflow.  \nHere, CNAPS means a **synapse‑like branching network** of AI models working together—not a simple linear pipeline.\n\n---\n\n## 1. Core Task\n\nThe user wants to restore the blurry face in their travel photos to appear sharp and naturally clear.  \nSub-goals include:\n- **Structure Preservation**: Maintain natural facial structure and geometry.\n- **Texture Realism**: Produce skin textures that are clear but not overly smoothed.\n- **Identity Consistency**: Ensure the restored face still resembles the user.\n- **Background Integrity**: The rest of the image should remain undistorted or enhanced appropriately.\n\n---\n\n## 2. CNAPS-style Workflow\n\n---\n\n### A. High-Level Overview\n\nThe system ingests a blurry travel photo. The image first undergoes global deblurring to remove motion blur. The face is detected and masked using MTCNN and BiSeNet. Based on the mask, the system splits the image into two regions: face and background. The face is processed using two parallel models: GFPGAN (for structure) and CodeFormer (for detail and identity). These outputs are fused. The background is enhanced separately via Real-ESRGAN. Finally, the fused face and enhanced background are blended using the soft mask to produce the final result.\n\n---\n\n### B. Visual Flow Diagram (block-style, text-based)\n\n```\ngraph TD\n    A[INPUT: Blurry Travel Photo] --> B[Deblur-MSSNet-RealBlurR: Global Deblur]\n    B --> C[Face Detection: MTCNN]\n    B --> D[Face Mask: BiSeNet]\n    C --> E[Crop Face Region]\n    D --> F[Generate Soft Mask]\n\n    E --> G1[GFPGANv1.4: Structure Restoration]\n    E --> G2[CodeFormer: Detail Restoration]\n\n    G1 --> H[Blend GFPGAN + CodeFormer Output]\n    G2 --> H\n\n    B --> I[Real-ESRGAN: Background Enhancement (Inverted Mask)]\n\n    H --> J[Reintegrate Face into Background using Mask]\n    I --> J\n\n    J --> K[OUTPUT: Restored Photo]\n```\n\n---\n\n### C. Justification of Design and Model Choices\n\n- **Deblur-MSSNet-RealBlurR**: Trained on real-world blurred images (RealBlurR dataset). Essential for removing global blur before fine restoration.  \n  → [MSSNet Paper](https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf)\n\n- **MTCNN**: Provides bounding box and facial landmark detection for accurate cropping.  \n- **BiSeNet**: Outputs semantic segmentation mask for precise face/background separation.  \n- **GFPGAN**: Strong prior-based model for restoring geometric structure of faces. Useful when blur is severe.  \n- **CodeFormer**: Complementary to GFPGAN—excels in preserving identity and realistic details.  \n- **Real-ESRGAN**: Applied to non-face regions to enhance clarity and match the quality of the restored face.  \n\nThis modular design follows a synaptic structure where different branches process specialized tasks and are merged at the end, creating a high-fidelity, identity-preserving result.",
    "llm_c": "As an expert AI scientist and architect, I will design a CNAPS-style workflow to address the user's request for face restoration in their travel photos.\n\n---\n\n## 1. Core Task\n\nThe user wants to restore their blurry face in a photograph to be sharp and natural.\n\nThis involves several sub-goals:\n*   **Structure Preservation:** The fundamental contours of the face (jawline, nose, eyes) must be restored accurately.\n*   **Texture Realism:** The skin must have a natural texture and tone, avoiding an overly smooth or artificial look.\n*   **Identity Preservation:** The restored face must remain faithful to the user's actual appearance and not look like a generic or different person.\n*   **Seamless Integration:** The restored face must blend perfectly with the rest of the image without jarring artifacts.\n\n---\n\n## 2. CNAPS-style Workflow\n\nThis workflow is designed as a synaptic network. It begins with a common pre-processing step, then branches into parallel pathways for specialized processing (face vs. background). The face pathway itself contains another internal branch to leverage the complementary strengths of two different restoration models. Finally, all paths are merged for the final output.\n\n---\n\n### A. High-Level Overview\n\nThe system takes the user's blurry travel photo as input.\n\n1.  **Initial Deblurring:** The entire image is first passed through a dedicated deblurring model, **Deblur-MSSNet-RealBlurR**, which is trained on real-world blur. This globally reduces motion or focus blur, providing a better starting point for all subsequent modules.\n\n2.  **Analysis & Branching (Face vs. Background):** The deblurred image is then analyzed. A face detector (**MTCNN**) locates the face, and a face parser (**BiSeNet**) generates a precise semantic mask of the facial region. This mask acts as a signal, splitting the workflow into two main branches:\n    *   **Face Restoration Branch:** This path focuses exclusively on the facial region defined by the mask.\n    *   **Background Enhancement Branch:** This path processes everything *outside* the facial mask.\n\n3.  **Synaptic Face Restoration (Internal Branching):** Inside the face branch, the cropped facial image is sent to **two models in parallel** to harness their unique strengths:\n    *   **Path A (Structure via GFPGAN):** The face is processed by **GFPGANv1.4**. This model uses a strong generative prior to robustly reconstruct facial structure and contours, even from severe blur.\n    *   **Path B (Fidelity via CodeFormer):** The same face is also processed by **CodeFormer**. This model excels at preserving the original identity and creating natural, high-fidelity skin textures.\n\n4.  **Merging & Fusion:** The outputs from the two face restoration paths are then merged. The structurally sound output from GFPGAN is blended with the high-fidelity output from CodeFormer. This fusion creates a result that is both structurally correct and naturally detailed. In the background branch, the non-face regions are enhanced using **Real-ESRGAN** to improve their clarity and match the restored face's quality.\n\n5.  **Final Output:** Finally, the fused, high-quality face is seamlessly composited back into the enhanced background using the soft-edged mask from BiSeNet. This produces the final, naturally restored photograph.\n\n---\n\n### B. Visual Flow Diagram (block-style, text-based)\n\n```mermaid\ngraph TD\n    A[INPUT: Blurry Travel Photo] --> B{PRE-PROCESSING};\n    B --> C[Deblur-MSSNet-RealBlurR: Global Deblur];\n    C --> D{ANALYSIS & MASKING};\n    D --> E[MTCNN: Detect Face BBox];\n    D --> F[BiSeNet: Generate Face Mask];\n\n    subgraph CNAPS Workflow\n        F --> G[BRANCH 1: FACE RESTORATION];\n        F --> H[BRANCH 2: BACKGROUND ENHANCEMENT];\n\n        subgraph Face Restoration Branch\n            G --> I[PATH 1A: GFPGANv1.4<br/>(Input: Cropped Face)];\n            G --> J[PATH 1B: CodeFormer<br/>(Input: Cropped Face)];\n            I --> K{MERGE & FUSE};\n            J --> K;\n        end\n\n        subgraph Background Enhancement Branch\n            H --> L[Real-ESRGAN<br/>(Input: Full Image + Inverted Mask)];\n        end\n\n        K --> M{FINAL COMPOSITION};\n        L --> M;\n    end\n\n    M -- Using BiSeNet Mask --> N[OUTPUT: Restored Photo];\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style N fill:#ccf,stroke:#333,stroke-width:2px\n```\n\n---\n\n### C. Justification of Design and Model Choices\n\nThis CNAPS design is explicitly tailored to the user's request and leverages the unique strengths of the provided models.\n\n1.  **Initial Deblurring (`Deblur-MSSNet-RealBlurR`):**\n    *   **Reasoning:** Travel photos often suffer from unpredictable, real-world motion and out-of-focus blur, not synthetic blur.\n    *   **Justification:** The **MSSNet paper** ([https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf](https://cg.postech.ac.kr/Research/MSSNet/MSSNet.pdf)) describes a \"Multi-Scale Selective Network\" architecture. The models trained on the `RealBlur` datasets (`RealBlurR` and `RealBlurJ`) are specifically designed for deblurring images from the wild. Using `Deblur-MSSNet-RealBlurR` as a pre-processing step simplifies the task for the downstream face-specific models, allowing them to focus on detail and structure rather than gross motion correction.\n\n2.  **Analysis and Branching (`MTCNN`, `BiSeNet`):**\n    *   **Reasoning:** The user's primary concern is their face. Treating the face and background separately allows for the use of specialized models for each, preventing the face restorer from altering the background and vice-versa."
  },
  "votes": {
    "gpt-4o": "llm_c",
    "claude-sonnet-4-20250514": "llm_c",
    "gemini-2.5-pro": "llm_c"
  },
  "majority_vote": "llm_c",
  "rationales": {
    "gpt-4o": "This response excels in clarity, completeness, and reasoning. The CNAPS-style workflow is innovative and uses the models effectively. The justification for each design decision is thorough, making it the most comprehensive and well-articulated response.",
    "claude-sonnet-4-20250514": "Delivers the most comprehensive and well-organized response. Excellent clarity with proper section headers, detailed explanations, and a sophisticated CNAPS design featuring true parallel processing with internal branching. The mermaid diagram enhances understanding, and the justification section provides thorough reasoning for each model choice. Most complete coverage of all required elements with superior readability and technical depth.",
    "gemini-2.5-pro": "This response is the clear winner. It proposes the same excellent workflow as llm_b but executes the explanation flawlessly. The clarity is outstanding, with a step-by-step narrative overview, a superior `mermaid` diagram that visually separates the branches, and an exceptionally detailed justification for every model choice. It directly connects the model capabilities to the user's sub-goals (e.g., structure, realism, identity), demonstrating a deep understanding of both the problem and the tools. It earns a perfect score across all criteria."
  },
  "scores": {
    "gpt-4o": {},
    "claude-sonnet-4-20250514": {},
    "gemini-2.5-pro": {
      "llm_a": 39,
      "llm_b": 48,
      "llm_c": 50
    }
  },
  "best_by_score": "llm_c"
}