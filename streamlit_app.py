import streamlit as st
import os
import json
import re
from pathlib import Path

# ğŸ”§ ì‚¬ìš©ì ì •ì˜: JSON í´ë” ê²½ë¡œ
DATA_PATH = Path("results")

# ğŸ” ëª¨ë¸ í‚¤ â†’ ì´ë¦„ ë§¤í•‘
MODEL_MAP = {
    "llm_a": "claude_sonnet4",
    "llm_b": "chatgpt4o",
    "llm_c": "gemini_pro"
}

# ğŸ“ í˜ì´ì§€ ì„¤ì •
st.set_page_config(layout="wide", page_title="AgentAI Viewer", page_icon="ğŸ§ ")

# âœ… ğŸ“ í˜ì´ì§€ ì„ íƒ ì¶”ê°€
st.sidebar.markdown("### ğŸ“ í˜ì´ì§€ ì„ íƒ")
page = st.sidebar.radio("í˜ì´ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”", [
    "ì‘ë‹µ ë¹„êµ ë³´ê¸°",
    "í–¥í›„ í”¼ë“œë°± ë°©í–¥ì„±",
    "ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ",
    "prompt_regenerated ë¹„êµ"
])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… 1. ì‘ë‹µ ë¹„êµ ë³´ê¸°ã„´ã„´ã„´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if page == "ì‘ë‹µ ë¹„êµ ë³´ê¸°":
    json_files = sorted(DATA_PATH.glob("*.json"))
    query_map = {f.stem: f for f in json_files}

    st.sidebar.title("ğŸ” Query Navigation")
    selected_query = st.sidebar.selectbox("Select Query", list(query_map.keys()))

    with st.sidebar.expander("ğŸ“‹ í‰ê°€ ê¸°ì¤€ ë³´ê¸°"):
        st.markdown("""
**ğŸ§ª í‰ê°€ ê¸°ì¤€ (ê° í•­ëª©ë‹¹ 10ì  ë§Œì )**

| ë²ˆí˜¸ | í•­ëª© | ì„¤ëª… |
|------|------|------|
| 1 | ëª…í™•ì„± ë° ê°€ë…ì„± | ì„¤ëª…ì´ ëª…í™•í•˜ê³  êµ¬ì¡°ì ìœ¼ë¡œ ì˜ ì •ë¦¬ë˜ì—ˆëŠ”ê°€? |
| 2 | ì •í™•ì„± ë° ì™„ì „ì„± | ìš”êµ¬ëœ ëª¨ë“  í•­ëª©ì´ ë¹ ì§ì—†ì´ í¬í•¨ë˜ì—ˆëŠ”ê°€? |
| 3 | CNAPS ìŠ¤íƒ€ì¼ ì›Œí¬í”Œë¡œìš° | ë¶„ê¸°(branch), ë³‘í•©(merge) ë“± ì‹œëƒ…ìŠ¤ êµ¬ì¡°ê°€ ë°˜ì˜ë˜ì—ˆëŠ”ê°€? |
| 4 | ì œê³µ ëª¨ë¸ë§Œ ì‚¬ìš© | ë¬¸ì œì—ì„œ ì œì‹œí•œ ëª¨ë¸ë§Œì„ ì‚¬ìš©í–ˆëŠ”ê°€? |
| 5 | í•´ì„ ê°€ëŠ¥ì„±ê³¼ ì„¤ë“ë ¥ | ì„ íƒ ëª¨ë¸ì˜ ê·¼ê±°ì™€ ì„¤ëª…ì´ ì„¤ë“ë ¥ ìˆì—ˆëŠ”ê°€? |
""")

    with open(query_map[selected_query], 'r') as f:
        data = json.load(f)

    query_text = data.get("query_text", "")
    responses = data.get("responses", {})
    votes = data.get("votes", {})
    majority = data.get("majority_vote", "")

    ask_match = re.search(r"A user asks:\n[\"â€œ](.+?)[\"â€]", query_text, re.DOTALL)
    user_ask = ask_match.group(1).strip() if ask_match else "(ì‚¬ìš©ì ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.)"

    model_block_match = re.search(r"### Recommended AI Models:\s*\n(.+)", query_text, re.DOTALL)
    models_raw = model_block_match.group(1).strip() if model_block_match else "(ëª¨ë¸ ëª©ë¡ ì—†ìŒ)"
    models_clean = re.findall(r"- \*\*(.*?)\*\*\n\s*Paper: (.*)", models_raw)
    models_md = "\n".join([f"- **{name}**\n  Paper: {link}" for name, link in models_clean]) if models_clean else models_raw

    st.markdown("## ğŸ™‹ ì‚¬ìš©ì ì§ˆë¬¸")
    st.info(f"**\"{user_ask}\"**")

    st.markdown("## ğŸ§  ì¶”ì²œëœ AI ëª¨ë¸ ëª©ë¡")
    st.code(models_md, language="markdown")

    st.markdown("## ğŸ¤– Model Responses")
    for raw_key in ["llm_a", "llm_b", "llm_c"]:
        response = responses.get(raw_key, "(No response found)")
        mapped_name = MODEL_MAP.get(raw_key, raw_key)
        voted_by = [model for model, v in votes.items() if v == raw_key]
        majority_flag = "ğŸŒŸ **Majority Vote**" if raw_key == majority else ""

        with st.expander(f"ğŸ§  {mapped_name}", expanded=True):
            st.markdown(response, unsafe_allow_html=True)
            st.markdown(f"âœ… **Voted by**: {', '.join(voted_by) if voted_by else 'None'}")
            if majority_flag:
                st.markdown(majority_flag)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… 2. í–¥í›„ í”¼ë“œë°± ë°©í–¥ì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
elif page == "í–¥í›„ í”¼ë“œë°± ë°©í–¥ì„±":
    st.title("ğŸ“ˆ í–¥í›„ í”¼ë“œë°± ë°©í–¥ì„±: RAG ì„±ëŠ¥ í–¥ìƒ ì¤‘ì‹¬ ì „ëµ")

    st.markdown("""
---

## ğŸ¯ ëª©í‘œ ì¬ì •ì˜

ê¸°ì¡´ì—ëŠ” ì¶œë ¥ ì–‘ì‹ì˜ êµ¬ì¡°í™”ì™€ CNAPS ìŠ¤íƒ€ì¼ ì‘ë‹µ ì¼ê´€ì„±ì— ì´ˆì ì„ ë§ì·„ë‹¤ë©´,  
**í–¥í›„ì—ëŠ” Retrieval-Augmented Generation (RAG) ì„±ëŠ¥ í–¥ìƒ ìì²´ë¥¼ í•µì‹¬ ëª©í‘œë¡œ ì „í™˜**í•©ë‹ˆë‹¤.

---

## ğŸ§  í•µì‹¬ ì „ëµ

- ë‹¨ìˆœíˆ ì–‘ì‹ì„ ì˜ ë§ì¶”ëŠ” ê²ƒì— ê·¸ì¹˜ì§€ ì•Šê³ ,  
  **ì •í™•í•˜ê³  ì •ë³´ ê¸°ë°˜ì˜ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” retrieval-aware fine-tuning** ë˜ëŠ”  
  **ê°•ê±´í•œ multi-hop retrieval chain í•™ìŠµ êµ¬ì¡°**ë¡œ ë°œì „

---

## ğŸ“˜ ì°¸ê³  ì‚¬ë¡€ (ì„±ëŠ¥ ì¤‘ì‹¬ RAG ì—°êµ¬)

- [CoRAG (Chain-of-Retrieval)](https://arxiv.org/pdf/2501.14342): step-by-step ê²€ìƒ‰ íë¦„ì„ ìƒì„±í•˜ë©° ì‘ë‹µ í’ˆì§ˆ ê·¹ëŒ€í™”
- RbFT: retrieval ì˜¤ë¥˜ì— ê°•ê±´í•œ fine-tuning êµ¬ì¡°
- Invar-RAG / OpenRAG: retrieverì™€ generatorë¥¼ end-to-endë¡œ ë™ì‹œ ìµœì í™”
- IterKey: sparse retrievalì—ì„œë„ iterative ë°©ì‹ìœ¼ë¡œ ì„±ëŠ¥ ë³´ê°•

---

## âœ… ìš”ì•½

ì•ìœ¼ë¡œëŠ” **LLMì´ ë‹¨ìˆœ ìƒì„±ìê°€ ì•„ë‹ˆë¼, ëŠ¥ë™ì  ê²€ìƒ‰ í–‰ìœ„ìë¡œ ì‘ë™í•  ìˆ˜ ìˆë„ë¡**  
retrievalì„ í¬í•¨í•œ í•™ìŠµ êµ¬ì¡°ì™€ í‰ê°€ ë°©ì‹ì„ ê°•í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í”¼ë“œë°±ê³¼ ê°œì„ ì„ ì´ì–´ê°ˆ ì˜ˆì •ì…ë‹ˆë‹¤.
""")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… 3. ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ (ì¸ë±ìŠ¤ ê¸°ë°˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
elif page == "ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ":
    st.title("ğŸ“Š LLM ì˜ˆì¸¡ ê²°ê³¼ vs ì •ë‹µ ë¹„êµ")

    pred_file_path = "results/generated_predictions_extracted.json"

    if os.path.exists(pred_file_path):
        with open(pred_file_path, "r", encoding="utf-8") as f:
            prediction_data = json.load(f)

        st.sidebar.markdown("### ğŸ”¢ í•­ëª© ì„ íƒ")
        example_idx = st.sidebar.number_input("ì¸ë±ìŠ¤ ì„ íƒ", min_value=0, max_value=len(prediction_data)-1, value=0, step=1)
        example = prediction_data[example_idx]

        st.markdown("## ğŸ™‹ Prompt (User Input)")
        st.code(example["prompt"], language="markdown")

        st.markdown("## ğŸ¤– Predict (LLM Output)")
        st.success(example["predict"])

        st.markdown("## âœ… Label (Ground Truth)")
        st.info(example["label"])
    else:
        st.warning("ğŸ“ results/generated_predictions_extracted.json íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… 4. Prompt Regenerated ë¹„êµ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
elif page == "prompt_regenerated ë¹„êµ":
    st.title("ğŸ“ Query1: Prompt (vanilla vs regenerated) ë¹„êµ")

    # JSON ê²½ë¡œ
    prompt_json_path = Path("results/prompt_comparsion.json")
    if not prompt_json_path.exists():
        st.warning("ğŸ“ results/merged_query1.json íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        with open(prompt_json_path, "r", encoding="utf-8") as f:
            prompt_data = json.load(f)

        # ëª¨ë¸ ì„ íƒ
        st.sidebar.markdown("### ğŸ§  ëª¨ë¸ ì„ íƒ")
        model_names = sorted(prompt_data.keys())
        selected_model = st.sidebar.selectbox("ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”", model_names)

        # í•´ë‹¹ ëª¨ë¸ì˜ vanilla / regenerated ì¿¼ë¦¬
        vanilla_query = prompt_data[selected_model].get("vanilla", "(vanilla ì¿¼ë¦¬ ì—†ìŒ)")
        regenerated_query = prompt_data[selected_model].get("regenerated", "(regenerated ì¿¼ë¦¬ ì—†ìŒ)")

        st.markdown(f"### ğŸ“Œ ì„ íƒí•œ ëª¨ë¸: `{selected_model}`")

        col1, col2 = st.columns(2)
        with col1:
            st.markdown("#### ğŸ§¾ Vanilla Prompt")
            st.code(vanilla_query, language="markdown")
        with col2:
            st.markdown("#### âœ¨ Regenerated Prompt")
            st.code(regenerated_query, language="markdown")
